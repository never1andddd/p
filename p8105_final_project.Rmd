---
title: "p8105_final_project"
output: html_document
---
# Motivations
Nowadays, most customers will search for restaurants on Yelp to decide where to eat for the next meal. The review and rating of a restaurant is of great importance and will help a customer to decide whether this is a good place to eat. Our goal is to find what property makes a restaurant popular and what drives a customer to choose a certain restaurant. Therefore, we choose to explore data of Yelp since it has reviews from customers all over the world.  
In this project, we have a map of restaurant showing the distribution of restaurants in different categories and some plots to reflect the relationship between customer flow(popularity) and category of a restaurant. We did some regression to help predict customer flow(popularity) of a restaurant. And  We also include some analysis of the review of a restaurant. This might help us to decide what a customer mostly looks for when choosing a restaurant.

# Related work
Things we discussed in class:  Data Wrangling II - Tidy Text http://jeffgoldsmith.com/DSI/tidy_text.html  
Interactivity - Plot.ly and flexdashboards  http://jeffgoldsmith.com/DSI/plotly_and_dashboards.html  
Things we learned in Biostatistic Method I: Multiple linear regression  

# Initial questions 
The question we trying to answer at first is 'what words are frequently mentioned in the negative and positive reviews?' and 'how does the category, location, opening hours and other attributes of a restaurant influence the customer flow?'  
In the course of our analysis, we are interested in whether we could recommend a restaurant for a customer and give some advice to a restaurant on how to improve their business.

# Data
The data used in this project is part of the Yelp Dataset Challenge. The dataset consists of a set of JSON files that include business information, reviews, tips, user information and check-ins. Variables in Business are business id, name, location, opening hours, category, average star rating, the number of reviews about the business and a series of attributes like noise level or reservations policy. Review objects contain star rating, the review text, the review date, and the number of votes that the review has received. In this project, we have focused on these two type of objects.  
Source: https://drive.google.com/drive/folders/190oLdoSyZVnydl9Jxth8G7aKzu3bDgee?usp=sharing  The original review json: https://drive.google.com/file/d/1wbIRfISw2ZW0zy4bZGtnRyhUEOImi5_H/view?usp=sharing  
The r code to clean the review json(we only extract 5000 reviews from this dataset because the original one is too large to process): https://drive.google.com/file/d/1EXrW_F_GmEq-tQlmtnYkMDwtt-IOA3tq/view?usp=sharing  
Scraping method: We use the stream_in function in R package "Jsonlite" to read in the data. Then We filtered the business by category to keep only those businesses in the restaurant category and in state of "AZ" (10219). We only extract first 5000 reviews from the original review json.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(httr)
library(tidyverse)
library(rjson)
library(tidyr)
library(readr)
library(dplyr)
library(jsonlite)
library(stringr)
library(forcats)
library(viridis)
library(tidytext)
library(janitor)
## Loading required package: viridisLite

library(plotly)
```

# Data clean
For business json: 1. We read in the json file 2. We extract price range from nested column "attributes" and get rid of other attributes 3. We obtain the opening hours of a restaurant on weekdays and weekends 4. We assign a category to a restaurant base on the frequency of the category.  
For review json: 1. We read in the review csv 2. We broke review text into words 3.We remove meaningless words using stop word in R package Tidytext 
## Business json
### Getting Data of Restaurant in AZ from business.json
```{r,message = FALSE , warning = FALSE }
# read in the business.json
business <- stream_in(file("../data/business.json"))

# specificly we are only interested in the covariate 'price' in the attributes column
price = select(business$attributes,RestaurantsPriceRange2)

business_no_attri = select(business,-attributes,-hours)

business.dat = bind_cols(business_no_attri,price)

# for simplicity, we focus on restaurants in state of Arizona(AZ)
rest.dat = business.dat %>%
  unnest(categories) %>%
  filter(str_detect(categories, "Restaurants")) %>%
  filter(state == "AZ") %>%
  select(business_id)

rest.dat = inner_join(rest.dat,business.dat)
# the rest.dat contains only restaurants in Arizona 

```

### Getting the opening hours of a restaurant in AZ
```{r buisness time}
business_time =  business %>% 
  select(business_id) %>% 
  rowid_to_column()
hours = business$hours %>% 
  rowid_to_column()

businesstime = left_join(business_time, hours)%>%
  na.omit()%>%
  separate(Monday,into = c("open1","close1"), sep = "-")%>%
  mutate(open1 = str_replace(open1, ":", "."),
         close1 = str_replace(close1, ":", ".") )
  
businesstime = businesstime%>%
  separate(Tuesday,into = c("open2","close2"), sep = "-")%>%
  mutate(open2 = str_replace(open2, ":", "."),
         close2 = str_replace(close2, ":", ".") )

businesstime = businesstime%>%
  separate(Wednesday,into = c("open3","close3"), sep = "-")%>%
  mutate(open3 = str_replace(open3, ":", "."),
         close3 = str_replace(close3, ":", ".") )

businesstime = businesstime%>%
  separate(Thursday,into = c("open4","close4"), sep = "-")%>%
  mutate(open4 = str_replace(open4, ":", "."),
         close4 = str_replace(close4, ":", ".") )

businesstime = businesstime%>%
  separate(Friday,into = c("open5","close5"), sep = "-")%>%
  mutate(open5 = str_replace(open4, ":", "."),
         close5 = str_replace(close4, ":", ".") )

businesstime = businesstime%>%
  separate(Saturday,into = c("open6","close6"), sep = "-")%>%
  mutate(open6 = str_replace(open6, ":", "."),
         close6 = str_replace(close6, ":", ".") )

businesstime = businesstime%>%
  separate(Sunday,into = c("open7","close7"), sep = "-")%>%
  mutate(open7 = str_replace(open7, ":", "."),
         close7 = str_replace(close7, ":", ".") )



library(magrittr)
cols = c(3:16)
businesstime[,cols] %<>% lapply(function(x) as.numeric(as.character(x)))

time = businesstime%>%
  mutate(mon = close1-open1,
         tue = close2 -open2,
         wed = close3-open3,
         thurs = close4 -open4,
         fri = close5-open5,
         sat = close6-open6,
         sun = close7-open7)

z=function(x){
  if(is.numeric(x)){
    ifelse(x<0,x+24,x)
  } 
}


a = time %>% select(-rowid, -business_id)
 b = map_df(a,z)

b=b%>%
  rowid_to_column()
c = time%>%
  select(rowid, business_id)

time = left_join(c, b) %>%
  mutate(weekdays = (mon+tue+wed+thurs+fri)/5,weekend= (sat+sun)/2) %>%
  select(weekdays,weekend,business_id)

cate.dat = inner_join(rest.dat,time,by="business_id")
# cate.dat contains the restaurants in Arizona with variables in rest.dat and their opening hours on weekdays and weekend.
```


### Assign a category to a restaurant
#### a rough preview of data
```{r}
cate.dat = cate.dat %>%
  unnest(categories)

# get an overview of categories, for simplicity, we are only showing 50 categories
cate.dat %>%
  count(categories, sort = TRUE) %>%
  top_n(50)%>%
  mutate(categories = fct_reorder(categories, n)) %>% 
  ggplot(aes(x = categories, y = n)) + 
  geom_bar(stat = "identity", fill = "blue", alpha = .6) + 
  coord_flip()
```
  
 We notice that:
 1. 'Restaurants', 'food', 'fast food', 'bars', 'sports bars', 'Event Planning & Serving' gives no specific information of what a restaurant serves and will cause a restaurant to have several categories. Therefore, we need to remove them.  
 2. Category 'bars' and 'nightlife' are somewhat overlaid. Therefore, we remove 'night life'.  

#### Extract top 20 common categories for simplicity of analysis
```{r}
# remove categories that do not provide useful information of what a restaurant serves
categories <- c('Restaurants','Food','Nightlife','Event Planning & Services','Sports Bars','Bars','Fast Food')
stop_cates = data.frame(categories) %>%
  mutate(categories = as.character(categories))

cate.dat = 
  anti_join(cate.dat, stop_cates)
  
# categories dataset contains top 20 common categories   
categories = cate.dat %>%
  count(categories, sort = TRUE) %>%
# omit categories with few restaurants, this is a trade-off because we might lose some restaurant, but we will get rid of some noise too
  top_n(20)

# get an overview of categories dataset
categories %>%
  mutate(categories = fct_reorder(categories, n)) %>% 
  ggplot(aes(x = categories, y = n)) + 
  geom_bar(stat = "identity", fill = "blue", alpha = .6) + 
  coord_flip()
# We could see that the remaining categories all provide specific information for a restaurant. There are most restaurants with category "American(traditional)" and numbers of restaurant of "Mexican", "Sandwiches" and "Pizza" are close. 

# final.dat dataset contains restaurants in top 20 common categories, but a restaurant might have multiple categories
final.dat = inner_join(categories,cate.dat) 
final.dat %>%
  unique() %>%
  distinct(business_id)


# assign the category with the highest frequency to each restaurant
final.dat = final.dat %>%
  group_by(business_id) %>%
  mutate(category=categories[1],n=n[1])%>%
  ungroup()%>%
  select(-categories)%>%
  unique()
#  
```
### Conclusion of the data clean of business json 
Comparing to the original dataset, in final.dat, each restaurant is assigned a category and opening hours on weekdays and weekends, the nested columns "attributes" and "opening hours" are removed. We will use for the visualization and analysis of data.
## Review json



# Exploratory analysis of business json (Visualization)
## A map of restaurants showing the distribution of categories 
```{r}
final.dat %>%
  mutate(text_label = str_c('Name:',name,"\nPostal code: ", postal_code, '\nAddress: ', address, '\nCategory: ', category)) %>% 
  plot_ly(x = ~longitude, y = ~latitude, type = "scatter", mode = "markers",
          alpha = 0.5, 
          color = ~category,
          text = ~text_label)

``` 
### Analysis 

## A boxplot showing the distribution of review count of 10 most common categories
```{r}
# We’re going to show only 10 categories with the most restaurants
common_category =
  final.dat %>% 
  count(category, sort = TRUE) %>% 
  top_n(10) %>% 
  select(category)
## Selecting by n

inner_join(final.dat, common_category,
             by = "category") %>% 
  mutate(category = fct_reorder(category, review_count)) %>% 
  plot_ly(y = ~review_count, color = ~category, type = "box",
          colors = "Set2")

```
### Analysis

## A bar chart showing the number of restaurants in each category
```{r}
final.dat%>% 
  mutate(category = fct_reorder(category, n)) %>% 
  plot_ly(x = ~category, y = ~n, color = ~category, type = "bar")

```
### Analysis 

## Bubble plot showing the distribution of ratings across different categories
```{r bubble plot}
#A grid of detailed average ratings by categories
#Basically, the average review rating scores in each state was reclassified from 1.0 to 5.0 by 0.5 increase. For visualization purpose, percentage of rating score is weighted.
dataWeighted_catestar <- final.dat %>% 
  group_by(category,stars) %>%
  summarise(totalByStar = n()) %>% arrange(desc(stars)) %>% 
  mutate(total = sum(totalByStar)) %>% mutate(percent = round((totalByStar / total)*100, 1)) %>%
  mutate(percentWeight = ifelse(percent >= 20, percent * 1.5, # custom column to weight the percent for size on the plot
                                ifelse(percent < 20 & percent >= 15, percent * 1, 
                                       ifelse(percent < 15 & percent >= 10, percent,
                                              ifelse(percent < 10 & percent >= 5, percent * 0.6, 1)))))


  library(ggplot2)
ggplot(dataWeighted_catestar, aes(x = category, y = stars, label = percent)) + 
    geom_point(aes(size = percentWeight * 0.8, colour = stars, alpha = 0.05)) + 
    geom_text(hjust = 0.4, size = 4) + scale_size(range = c(1, 30), guide = "none") + 
    scale_color_gradient(low = "darkblue", high = "red") + labs(title = "A grid of detailed avg.ratings by category ", 
    x = "Category", y = "Detailed Avg.Ratings") + scale_y_continuous(breaks = seq(1, 
    5, 0.5)) + theme(legend.title = element_blank())


city <- final.dat %>% 
  group_by(city) %>% 
  summarise(totalCity = n()) %>% 
  top_n(10)
dataWeighted_city <- left_join(city, final.dat) %>% 
  group_by(city,stars) %>%
  summarise(totalByStar = n()) %>% arrange(desc(stars)) %>% 
  mutate(total = sum(totalByStar)) %>% mutate(percent = round((totalByStar / total)*100, 1)) %>%
  mutate(percentWeight = ifelse(percent >= 20, percent * 1.5, # custom column to weight the percent for size on the plot
                                ifelse(percent < 20 & percent >= 15, percent * 1, 
                                       ifelse(percent < 15 & percent >= 10, percent,
                                              ifelse(percent < 10 & percent >= 5, percent * 0.6, 1)))))

ggplot(dataWeighted_city, aes(x = city, y = stars, label = percent)) + 
    geom_point(aes(size = percentWeight * 0.8, colour = stars, alpha = 0.05)) + 
    geom_text(hjust = 0.4, size = 4) + scale_size(range = c(1, 30), guide = "none") + 
    scale_color_gradient(low = "blue", high = "green") + labs(title = "A grid of detailed avg.ratings by city ", 
    x = "City", y = "Detailed Avg.Ratings") + scale_y_continuous(breaks = seq(1, 
    5, 0.5)) + theme(legend.title = element_blank())


```
### Analysis
**Bubble Plot"
Different colors represent different rating levels.
The number in the bubble represents the precentage of the specific rating level and this percentage can also be explained by the bubble size.

We can see from the bubble plot that there are 10 kinds of restaurants and nine levels of ratings, seperately shown in the y-axis and x-axis. Most restaurants have main rating level are around 4.0 and 3.5.The breakfast,Italian and new restaurants have the biggest percentage in 4.0. The precentage of each rating level of Hamburger Restaurants are around 15%, so they have more even distrutions.For breakfast restaurants, they have biggest 4.5 ratings percentage and more than 70% resturants have rating higher than 3.0 . Thus, we can conclude breakfats restaurants have highest rating levels among these 10 kinds of restaurants. However, wings restaurants have lowest percentage in 3.5 and 4.0 rating levels and over 50% of ratings below or equal to 3.0. Thus, wings restaurants shows the lowest rating.


In the second bubble plot, we choose 10 cities with most restuarants. The ratings is concentrated between 3.0 and 4.0. Percentage of rating 4.5 is around 10% in most cities. And find the resturants in Scottsdale have biggestt bubble in rating 4.0, meaning the city has highest percentage of 4.0 rating. Besides, in the lower rating level such as 2.0 and 2.5, city Goodyear have biggest bubbles among ten cities. Although there are some differences in every rating levels among cities, the variations are very extreme. Therefore, it seems that the mean rating levels among these cities would be similar. 





```{r}
city <- final.dat %>% 
  group_by(city) %>% 
  summarise(totalCity = n()) %>% 
  top_n(10)
dataWeighted_city <- left_join(city, final.dat) %>% 
  group_by(city,stars) %>%
  summarise(totalByStar = n()) %>% arrange(desc(stars)) %>% 
  mutate(total = sum(totalByStar)) %>% mutate(percent = round((totalByStar / total)*100, 1)) %>%
  mutate(percentWeight = ifelse(percent >= 20, percent * 1.5, # custom column to weight the percent for size on the plot
                                ifelse(percent < 20 & percent >= 15, percent * 1, 
                                       ifelse(percent < 15 & percent >= 10, percent,
                                              ifelse(percent < 10 & percent >= 5, percent * 0.6, 1)))))


ggplot(dataWeighted_city, aes(x = city, y = stars, label = percent)) + 
    geom_point(aes(size = percentWeight * 0.8, colour = stars, alpha = 0.05)) + 
    geom_text(hjust = 0.4, size = 4) + scale_size(range = c(1, 30), guide = "none") + 
    scale_color_gradient(low = "blue", high = "green") + labs(title = "A grid of detailed avg.ratings by city ", 
    x = "City", y = "Detailed Avg.Ratings") + scale_y_continuous(breaks = seq(1, 
    5, 0.5)) + theme(legend.title = element_blank())
```


# Exploratory statistical analysis of business json
## Regression
## Analysis 

# Exploratory analysis of review csv (visulization)
## bar chart
## density plot
## word cloud 

# Exploratory statistical analysis of review csv 

## Regression
## Analysis

## explore words

```{r}
review <- read_csv("../data/review_sample_5000.csv") %>% 
  clean_names() %>% 
  select(business_id, everything()) %>% 
  nest(x1:cool)

businessId = business.dat %>%
  mutate(categories = as.character(categories)) %>%
  filter(str_detect(categories, "Restaurants")) %>% 
  select(business_id)
review_business = inner_join(businessId, review, by  = "business_id") %>% 
  unnest()

```

## linear regression of reviews

```{r}
review_length = review_business %>%
  mutate(text = as.character(text),
         inspection_num = row_number()) %>%
  unnest_tokens(word, text) %>% 
  group_by(inspection_num) %>% 
  count() 
```

```{r}
#review$is_positive = as.factor(ifelse(review$stars >= 4, "1", "0"))

#split text, remove stop words
data("stop_words")
inspection_words = review_business %>%
  mutate(text = as.character(text),
         inspection_num = row_number()) %>% 
  unnest_tokens(word, text)
inspection_words = 
  anti_join(inspection_words, stop_words)

bing_sentiments = get_sentiments("bing")
#neg and pos numbers
inspection_sentiments = inspection_words %>% 
  inner_join(., bing_sentiments) %>% 
  count(inspection_num, sentiment) %>% 
  spread(sentiment, n, fill = 0) 
star = review_business %>%
  mutate(text = as.character(text),
         inspection_num = row_number()) %>% 
  select(inspection_num, stars)

inspection_lm = inner_join(star, inspection_sentiments, by = "inspection_num") %>% 
  inner_join(., review_length)
```

## fit a linear model

```{r}
reviews_reg = lm(stars ~ n + positive + negative, data = inspection_lm)
summary(reviews_reg)
```

#Charts
## Positivity Bar chart
```{r}
inspection_lm %>%
  filter(positive != 0) %>%
  mutate(stars = factor(stars),
         percent_pos = round((positive)/n,digits = 2)) %>%
  select(stars, percent_pos) %>% 
  group_by(percent_pos, stars) %>% 
  count() %>% 
 ggplot(aes(x = percent_pos, y = n, fill = stars)) +
  geom_col() +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::comma) +
  #theme_custom()+
  labs(title=paste("Yelp Review Negativity, by # of Stars for", format(nrow(inspection_lm),big.mark=","),"Reviews"), x="% Review Negativity (# Negative Words : # Words)", y="Total # of Reviews") +
  scale_fill_manual(breaks = c("1", "2", "3", "4", "5"), 
                       values = c("burlywood2", "violet", "violetred1", "violetred3", "deeppink4"))
```


## Positivity Density
```{r Positivity Density}  
inspection_lm %>%
  filter(positive != 0) %>%
  mutate(s =stars,
    percent_pos = round((positive)/n,digits = 2)) %>%
ggplot(aes(x=percent_pos, fill=as.factor(stars), color=as.factor(stars), y=..density..)) +
  geom_density(alpha = 1, position="fill") +
  scale_x_continuous(limits=c(0,0.43), label = scales::percent) +
  scale_y_continuous(label = scales::comma) +
  theme(legend.title = element_blank(), legend.position="top", legend.direction="horizontal", legend.key.width=unit(0.5, "cm"), legend.key.height=unit(0.25, "cm"), legend.margin=unit(-0.5,"cm"), panel.margin=element_blank()) +
  labs(title=paste("Yelp Positivity Proportion, by # Stars for", format(nrow(inspection_lm),big.mark=","),"Reviews"), x="% Review Positivity (# Positive Words : # Words)", y="Proportion of Reviews") +
  scale_fill_manual(values=c("burlywood2", "violet", "violetred1", "violetred3", "deeppink4"), labels = c("1 Star", "2 Stars", "3 Stars", "4 Stars", "5 Stars")) +
  scale_color_manual(values=c("burlywood2", "violet", "violetred1", "violetred3", "deeppink4"), labels = c("1 Star", "2 Stars", "3 Stars", "4 Stars", "5 Stars")) 

```

## Negativity Bar Chart
```{r}
inspection_lm %>%
  filter(negative != 0) %>%
  mutate(stars = factor(stars),
         percent_neg = round((negative)/n,digits = 2)) %>%
  select(stars, percent_neg) %>% 
  group_by(percent_neg, stars) %>% 
  count() %>% 
 ggplot(aes(x = percent_neg, y = n, fill = stars)) +
  geom_col() +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::comma) +
  #theme_custom()+
  labs(title=paste("Yelp Review Negativity, by # of Stars for", format(nrow(inspection_lm),big.mark=","),"Reviews"), x="% Review Negativity (# Negative Words : # Words)", y="Total # of Reviews") +
  scale_fill_manual(breaks = c("1", "2", "3", "4", "5"), 
                       values = c("burlywood2", "violet", "violetred1", "violetred3", "deeppink4"))
```

## Negativity Density
```{r Negativity Density}  
inspection_lm %>%
  filter(negative != 0) %>%
  mutate(s =stars,
    percent_neg = round((negative)/n,digits = 2)) %>%
ggplot(aes(x=percent_neg, fill=as.factor(stars), color=as.factor(stars), y=..density..)) +
  geom_density(alpha = 1, position="fill") +
  scale_x_continuous(limits=c(0,0.35), label = scales::percent) +
  scale_y_continuous(label = scales::comma) +
  theme(legend.title = element_blank(), legend.position="top", legend.direction="horizontal", legend.key.width=unit(0.5, "cm"), legend.key.height=unit(0.25, "cm"), legend.margin=unit(-0.5,"cm"), panel.margin=element_blank()) +
  labs(title=paste("Yelp Negativity Proportion, by # Stars for", format(nrow(inspection_lm),big.mark=","),"Reviews"), x="% Review Negativity (# Negative Words : # Words)", y="Proportion of Reviews") +
  scale_fill_manual(values=c("burlywood2", "violet", "violetred1", "violetred3", "deeppink4"), labels = c("1 Star", "2 Stars", "3 Stars", "4 Stars", "5 Stars")) +
  scale_color_manual(values=c("burlywood2", "violet", "violetred1", "violetred3", "deeppink4"), labels = c("1 Star", "2 Stars", "3 Stars", "4 Stars", "5 Stars")) 

```

##donut charts
```{r donut charts}
 data_reviews_agg <- inspection_lm %>%
 	group_by(stars) %>%
 	summarize(count = n()) %>%
 	mutate(fraction = count / sum(count),
 			ymax = cumsum(fraction),
 			ymin = c(0, head(ymax, n=-1)))
rank_colors = c("burlywood2", "violet", "violetred1", "violetred3", "deeppink4")
 
ggplot(aes(fill=as.factor(stars), color=as.factor(stars), ymax=ymax, ymin=ymin, xmax=5, xmin=4), data=data_reviews_agg) +
    geom_rect(color="white") +
     coord_polar(theta="y") +
     annotate("text", label = paste(format(data_reviews_agg$fraction * 100, digits=2),"%",sep=''), x=rep(6,5), y=(data_reviews_agg$ymin + data_reviews_agg$ymax)/2, col=rank_colors, size=3) +
     xlim(c(0, 6)) +
 theme(panel.grid=element_blank(), axis.text=element_blank(), axis.ticks=element_blank(), panel.background=element_blank(), axis.title.x = element_blank(), axis.title.y=element_blank(),legend.title = element_blank(),  legend.key.height=unit(0.25, "cm"), legend.margin=unit(-0.5,"cm"),panel.border= element_blank()) +
     scale_fill_manual(values=rank_colors, labels = c("1 Star", "2 Stars", "3 Stars", "4 Stars", "5 Stars")) +
  scale_color_manual(values=rank_colors, labels = c("1 Star", "2 Stars", "3 Stars", "4 Stars", "5 Stars")) +
     annotate("text", x = 0, y = 0, label = "Customized ring plot", col="#1a1a1a", family="Source Sans Pro Light", size=11)

```


##wordcloud
```{r wordcloud, eval=FALSE}

count_stop_words <- function(x) {
	word_array <- strsplit(as.character(x)," ")[[1]]
	 
	return (sum(word_array %in% stop_words))
}

inspection_words = inspection_words%>% 
  filter(lapply(word, count_stop_words) < 1)

star1 = inspection_words %>% filter(stars == 1)
star1_word = star1 %>%
  group_by(word)%>%
  count(word,sort = TRUE)
star5 = inspection_words %>% filter(stars == 5)
star5_word = star5 %>%
  group_by(word)%>%
  count(word,sort = TRUE)

 library(wordcloud)
pal <- brewer.pal(9, "Reds")
pal <- pal[-c(1:3)]
png(filename = "words-1star.png", width = 3000, height = 3000, res= 300)
wordcloud(toupper(star1_word$word), star1_word$n, scale=c(11,.1), random.order=F, rot.per=.10, max.words=5000, colors=pal, random.color=T)

dev.off()

pal <- brewer.pal(9, "Greens")
pal <- pal[-c(1:3)]
png(filename = "words-5star.png", width = 3000, height = 3000, res= 300)
wordcloud(toupper(star5_word$word), star5_word$n, scale=c(9,.1), random.order=F, rot.per=.10, max.words=5000, colors=pal, random.color=T)

dev.off()
library(png)
image_1star = readPNG("../data/words-1star.png") 
image_5star = readPNG("../data/words-5star.png") 
```
#linear regression

```{r model 3 variables selction}
linear = cate.dat1 %>%
  mutate(categories = str_replace(categories,"c\\(","")) %>%
  unnest_tokens(category, categories) %>%
  select(-neighborhood)

mod.categories = linear %>%
  count(category, sort = TRUE) %>%
  top_n(3)

mod.final = inner_join(mod.categories,linear) 

mod.final = mod.final %>%
  dplyr::filter(city == "Phoenix"|city == "Scottsdale"|city =="Mesa")

model = mod.final %>%
  dplyr::select(stars, category,city,review_count, RestaurantsPriceRange2,weekdays,weekend) %>%
  dplyr::mutate(category = as.factor(category),
         city = as.factor(city),
         stars = as.factor(stars))

ggplot(model, aes(x = stars)) + 
  geom_histogram()
ggplot(model, aes(x = review_count)) + 
  geom_histogram()

ggplot(model, aes(x = weekdays)) + 
  geom_histogram()
  
ggplot(model, aes(x = weekend)) + 
  geom_histogram()

model1 = model %>%
  mutate(log_review = log(review_count)) %>%
  dplyr::select(-review_count)

ggplot(model1, aes(x = log_review)) + 
  geom_histogram()



pairs(model1)

```

```{r}
model1 = model1%>%
  na.omit()

mult.fit = lm( log_review~ ., data=model1)
step(mult.fit, direction='backward')
summary (mult.fit)
#lm(formula = log_review ~ stars + category + city + RestaurantsPriceRange2 + weekdays + weekend, data = model1)

mult.fit1 = lm(log_review ~ stars + category + city + RestaurantsPriceRange2 + weekdays + weekend, data=model1)
summary(mult.fit1)
anova(mult.fit1)
#However, we find that in significant, p-value of weekend variable is greater than 0.05.

mult.fit3 = lm(log_review ~ stars + category + city + RestaurantsPriceRange2 + weekdays, data=model1)
anova(mult.fit3,mult.fit1)




res<-t.test(model1$weekdays, model1$weekend, var.equal=FALSE, paired=FALSE)               # var.equal=FALSE is the default, so no need to specifically write it.
res
```

**Conclusion---linear regression**
In this linear model, we consider reviews as the outcome and try to find the associations among those variables. However, we find the review variable is not normally distributed, so we transfrom it into log_review. At first, we clean the linear data and choose three most categories---bars, restaurants and food. The we use backward stepwise to build the model. Then, we get "log_review ~ stars + category + city + RestaurantsPriceRange2 + weekdays + weekend" model. In this model, city stars and catogory predictors are categorical and "weekdays","weekends" are continuous. However, in summary and general ANOVA Test, we find the coefficient of weekend are not significant. In this way, we use partial anova test, we assume that the null hypothesis is "log_review ~ stars + category + city + RestaurantsPriceRange2 + weekdays" and find the p-value is greater than 0.05 and we fail to reject null hypothesis and choose the small model. And this also satisfies "parsimony". 

In addition, through t test, we test the difference of business hours between weekdays and weekends and their business hours are significantly different.

# Discussion
What were your findings? Are they what you expect? What insights into the data can you make?

